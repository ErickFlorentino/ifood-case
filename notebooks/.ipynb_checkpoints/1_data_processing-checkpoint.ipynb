{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d403807e-bf15-4d23-8836-53837c5490e2",
   "metadata": {},
   "source": [
    "# MRO - Modelo de RecomendaÃ§Ã£o de Ofertas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a779b-8b0e-4fae-8342-2e901e7e2904",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6582fbe5-a1ee-4968-854d-4e5026cf8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ManipulaÃ§Ã£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Datas\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "# VisualizaÃ§Ã£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# EstatÃ­stica\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# PrÃ©-processamento\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, \n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder, \n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "# Modelagem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# AvaliaÃ§Ã£o de modelos\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ValidaÃ§Ã£o cruzada e otimizaÃ§Ã£o\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "# Agrupamento\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Salvamento de modelos\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1275f8-04e6-49a4-976d-dbb28112f952",
   "metadata": {},
   "source": [
    "## FunÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4077c3-6a74-43ff-9639-184ef9ea523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ã£o para ajustar datas de registro no perfil ---\n",
    "def ajustar_data(data_str):\n",
    "    ano = int(data_str[:4])\n",
    "    dia = int(data_str[6:8])\n",
    "    mes_atual = datetime.now().month\n",
    "\n",
    "    # Corrigir dia para o Ãºltimo dia vÃ¡lido do mÃªs atual\n",
    "    _, ultimo_dia = calendar.monthrange(ano, mes_atual)\n",
    "    dia_corrigido = min(dia, ultimo_dia)\n",
    "\n",
    "    return datetime(year=ano, month=mes_atual, day=dia_corrigido)\n",
    "\n",
    "# FunÃ§Ã£o para calcular diferenÃ§a em meses entre duas datas\n",
    "def meses_de_diferenca(inicio, fim):\n",
    "    return (fim.year - inicio.year) * 12 + (fim.month - inicio.month)\n",
    "\n",
    "\n",
    "# FunÃ§Ã£o para preencher valores ausentes por mÃ©dia de grupo\n",
    "def preencher_media_grupo(df, col, grupo):\n",
    "    media = df.groupby(grupo)[col].transform('mean')\n",
    "    return df[col].fillna(media)\n",
    "\n",
    "# FunÃ§Ã£o para treinar o KMeans, salvar artefatos e gerar resumo comportamental por cluster\n",
    "def treinar_kmeans(produto, X_train):\n",
    "    print(f'\\nTreinando modelo para: {produto}')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Encontrar melhor k pelo mÃ©todo do cotovelo\n",
    "    inertia = []\n",
    "    for k in range(2, 10):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(range(2, 10), inertia, 'bx-')\n",
    "    plt.title(f'MÃ©todo do cotovelo - {produto}')\n",
    "    plt.xlabel(\"NÃºmero de Clusters\")\n",
    "    plt.ylabel(\"InÃ©rcia\")\n",
    "    plt.show()\n",
    "\n",
    "    # Determinar o melhor k (ponto de maior curvatura)\n",
    "    diff = np.diff(inertia)\n",
    "    diff2 = np.diff(diff)\n",
    "    best_k = np.argmin(diff2) + 2\n",
    "    \n",
    "    print(f'Melhor k Real: {best_k}')\n",
    "    \n",
    "    #forÃ§ando\n",
    "    best_k = 3\n",
    "    \n",
    "    print(f'Melhor k: {best_k}')\n",
    "\n",
    "    # Treinar modelo final\n",
    "    modelo_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    modelo_final.fit(X_scaled)\n",
    "    X_train_clusterizado = X_train.copy()\n",
    "    X_train_clusterizado['cluster'] = modelo_final.labels_\n",
    "\n",
    "    # Salvar modelo e scaler\n",
    "    os.makedirs('modelos', exist_ok=True)\n",
    "    with open(f'modelos/{produto}_kmeans.pkl', 'wb') as f:\n",
    "        pickle.dump(modelo_final, f)\n",
    "    with open(f'modelos/{produto}_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Salvar centrÃ³ides\n",
    "    centroides = modelo_final.cluster_centers_\n",
    "    pd.DataFrame(centroides, columns=X_train.columns).to_csv(f'{produto}_centroides.csv', index=False)\n",
    "\n",
    "    # Calcular e salvar raio mÃ¡ximo por cluster\n",
    "    distancias = np.linalg.norm(X_scaled - modelo_final.cluster_centers_[modelo_final.labels_], axis=1)\n",
    "    raio_max = pd.Series(distancias).groupby(X_train_clusterizado['cluster']).max()\n",
    "    raio_max.to_csv(f'modelos/{produto}_raio_max.csv')\n",
    "\n",
    "    # Gerar descritivo comportamental por grupo com % da base\n",
    "    resumo = X_train_clusterizado.groupby('cluster').mean().round(2)\n",
    "    contagem = X_train_clusterizado['cluster'].value_counts(normalize=True).sort_index() * 100\n",
    "    resumo['perc_base'] = contagem.round(2)\n",
    "\n",
    "    resumo.to_csv(f'{produto}_resumo_cluster.csv')\n",
    "    print(\"Resumo comportamental dos clusters salvo como CSV.\")\n",
    "\n",
    "    print(\"Modelo, centrÃ³ides, raios e resumo salvos com sucesso.\")\n",
    "    \n",
    "    return modelo_final, scaler, centroides, raio_max, resumo\n",
    "\n",
    "\n",
    "# FunÃ§Ã£o para mostrar a importÃ¢ncia das variÃ¡veis para o melhor modelo\n",
    "def mostrar_importancia_variaveis(modelo, X):\n",
    "    if hasattr(modelo.named_steps['clf'], 'feature_importances_'):\n",
    "        importancias = modelo.named_steps['clf'].feature_importances_\n",
    "    elif hasattr(modelo.named_steps['clf'], 'coef_'):\n",
    "        # Para LogisticRegression (multiclasse): pegar a mÃ©dia absoluta dos coeficientes por feature\n",
    "        importancias = np.mean(np.abs(modelo.named_steps['clf'].coef_), axis=0)\n",
    "    else:\n",
    "        print(\"Modelo nÃ£o suporta feature_importances_ ou coef_\")\n",
    "        return\n",
    "    \n",
    "    # Criar dataframe para exibir importÃ¢ncias\n",
    "    df_importancias = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importancias\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(df_importancias)\n",
    "    \n",
    "    # Plotar as importÃ¢ncias\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(df_importancias['feature'], df_importancias['importance'])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"ImportÃ¢ncia das VariÃ¡veis\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- FunÃ§Ã£o de aprendizado supervisionado ajustada para multi-classes (offer_id) ---\n",
    "def aprendizado_supervisionado(X_train, X_test, y_train, y_test, df_metricas_por_grupo, X_test_geral):\n",
    "    modelos = {\n",
    "        'LogisticRegression': (\n",
    "            LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs'),\n",
    "            {'clf__C': [0.1, 1, 10]}\n",
    "        ),\n",
    "        'DecisionTree': (\n",
    "            DecisionTreeClassifier(random_state=42),\n",
    "            {'clf__max_depth': [None, 5, 10, 20], 'clf__min_samples_split': [2, 5, 10]}\n",
    "        ),\n",
    "        'RandomForest': (\n",
    "            RandomForestClassifier(random_state=42),\n",
    "            {'clf__n_estimators': [100, 200], 'clf__max_depth': [None, 5, 10]}\n",
    "        ),\n",
    "        'XGBClassifier': (\n",
    "            XGBClassifier(eval_metric='mlogloss', tree_method='hist', random_state=42),\n",
    "            {'clf__n_estimators': [100, 200], 'clf__max_depth': [3, 6], 'clf__learning_rate': [0.05, 0.1]}\n",
    "        )\n",
    "    }\n",
    "\n",
    "    melhores_modelos = {}\n",
    "    lista_metricas_por_grupo = []\n",
    "    \n",
    "    # DataFrame para salvar probabilidades (probabilidades para cada classe)\n",
    "    df_probabilidades = pd.DataFrame()\n",
    "    if 'num_telefone' in X_test_geral.columns:\n",
    "        df_probabilidades['num_telefone'] = X_test_geral['num_telefone']\n",
    "    df_probabilidades['true_offer_id'] = y_test.reset_index(drop=True)\n",
    "\n",
    "    for nome, (modelo, parametros) in modelos.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', modelo)\n",
    "        ])\n",
    "\n",
    "        grid = GridSearchCV(pipeline, param_grid=parametros, scoring='f1_weighted', cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        melhores_modelos[nome] = grid.best_estimator_\n",
    "\n",
    "        # Prever classes e probabilidades\n",
    "        y_pred = grid.predict(X_test)\n",
    "        y_pred_proba = grid.predict_proba(X_test)\n",
    "\n",
    "        # RelatÃ³rio por classe\n",
    "        relatorio = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True, zero_division=0)\n",
    "        df_relatorio = pd.DataFrame(relatorio).transpose()\n",
    "        \n",
    "        print(f'RelatÃ³rio de mÃ©tricas por classe para {nome}:')\n",
    "        print(df_relatorio)\n",
    "\n",
    "\n",
    "        # Adicionar colunas de probabilidade para cada classe no df_probabilidades\n",
    "        for idx, classe in enumerate(grid.classes_):\n",
    "            col_name = f'proba_{nome}_{classe}'\n",
    "            df_probabilidades[col_name] = y_pred_proba[:, idx]\n",
    "\n",
    "        # MÃ©tricas multi-classe\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f'ALGORITMO: {nome} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}')\n",
    "\n",
    "        lista_metricas_por_grupo.append({\n",
    "            'metodo': f'SUP. MULTI ({nome})',\n",
    "            'precision': round(precision, 4),\n",
    "            'recall': round(recall, 4),\n",
    "            'f1': round(f1, 4)\n",
    "        })\n",
    "\n",
    "    # Salvar probabilidades\n",
    "    df_probabilidades.to_csv('probabilidades_por_algoritmo_multiclasse.csv', index=False, sep=';')\n",
    "    print(f\"ğŸ’¾ Probabilidades salvas em: bases/probabilidades_por_algoritmo_multiclasse.csv\")\n",
    "\n",
    "    # Salvar o melhor modelo baseado na F1 score\n",
    "    melhor_modelo = max(lista_metricas_por_grupo, key=lambda x: x['f1'])\n",
    "    nome_melhor = melhor_modelo['metodo'].split('(')[-1].replace(')', '')\n",
    "    modelo_para_salvar = melhores_modelos[nome_melhor]\n",
    "    joblib.dump(modelo_para_salvar, 'melhor_modelo_f1_multiclasse.pkl')\n",
    "\n",
    "    # Concatenar mÃ©tricas ao df original\n",
    "    df_novos = pd.DataFrame(lista_metricas_por_grupo)\n",
    "    df_metricas_por_grupo = pd.concat([df_metricas_por_grupo, df_novos], ignore_index=True)\n",
    "        \n",
    "    print(f\"\\nâœ… Melhor modelo salvo: {nome_melhor} com F1-score {melhor_modelo['f1']:.4f}\")\n",
    "\n",
    "\n",
    "    # Depois de selecionar o melhor modelo, chame:\n",
    "    mostrar_importancia_variaveis(modelo_para_salvar, X_train_clean)\n",
    "    \n",
    "    return df_metricas_por_grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616f6c9-e492-4ba4-a0d6-d862ee781bca",
   "metadata": {},
   "source": [
    "## Engenharia de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a15681-5b43-4e52-9dc8-bc43b0b27a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  registered_on data_ajustada  tempo_de_casa\n",
      "0      20170212    2017-09-12             96\n",
      "1      20170715    2017-09-15             96\n",
      "2      20180712    2018-09-12             84\n",
      "3      20170509    2017-09-09             96\n",
      "4      20170804    2017-09-04             96\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definindo caminho base para os arquivos JSON ---\n",
    "caminho_arquivo = r'C:\\Users\\F8090385\\ifood\\raw\\\\'\n",
    "\n",
    "# --- 2. Carga arquivos JSON em DataFrames ---\n",
    "offers = pd.read_json(caminho_arquivo + 'offers.json')\n",
    "profile = pd.read_json(caminho_arquivo + 'profile.json')\n",
    "transactions = pd.read_json(caminho_arquivo + 'transactions.json')\n",
    "\n",
    "# --- 3. Normalizando nomes das colunas para consistÃªncia entre dataframes ---\n",
    "offers.rename(columns={'id': 'offer_id'}, inplace=True)\n",
    "profile.rename(columns={'id': 'account_id'}, inplace=True)\n",
    "\n",
    "# --- 4. Expandir coluna 'value' no dataframe transactions em mÃºltiplas colunas ---\n",
    "value_df = transactions['value'].apply(pd.Series)\n",
    "\n",
    "# --- 5. Processar transactions\n",
    "\n",
    "# Substituir coluna 'value' pelas colunas expandidas (ex: offer_id, reward, etc.)\n",
    "transactions_v2 = pd.concat([transactions.drop(columns=['value']), value_df], axis=1)\n",
    "\n",
    "# Corrigir inconsistÃªncia: quando 'offer_id' estiver vazio, usar 'offer id'\n",
    "transactions_v2['offer_id'] = transactions_v2['offer id'].combine_first(transactions_v2['offer_id'])\n",
    "\n",
    "# Remover a coluna antiga 'offer id'\n",
    "transactions_v2.drop(columns=['offer id'], inplace=True)\n",
    "\n",
    "# --- 6. Processar canais (channels) no dataframe offers\n",
    "\n",
    "# Se coluna 'channels' estiver em formato string, convertÃª-la para lista (separada por vÃ­rgulas)\n",
    "offers['channels'] = offers['channels'].apply(\n",
    "    lambda x: x.split(',') if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# \"Explodir\" a lista de canais para uma linha por canal (transforma lista em mÃºltiplas linhas)\n",
    "offers_expanded = offers.explode('channels')\n",
    "\n",
    "# Criar variÃ¡veis dummies (colunas binÃ¡rias) para cada canal\n",
    "channels_dummies = pd.get_dummies(offers_expanded['channels'])\n",
    "\n",
    "# Agrupar de volta por oferta somando dummies e limitando valores para 1 (evita duplicatas)\n",
    "channels_summary = channels_dummies.groupby(offers_expanded.index).sum().clip(upper=1)\n",
    "\n",
    "# Anexar variÃ¡veis dummy no dataframe original de ofertas\n",
    "offers = offers.join(channels_summary)\n",
    "\n",
    "\n",
    "# --- 7. Calcular tempo de casa (profile)\n",
    "\n",
    "# Garantir que 'registered_on' seja string para evitar erros de parsing\n",
    "profile['registered_on'] = profile['registered_on'].astype(str)\n",
    "\n",
    "# Obter mÃªs e ano atuais para cÃ¡lculo do tempo de casa\n",
    "mes_atual = datetime.now().month\n",
    "ano_atual = datetime.now().year\n",
    "\n",
    "# Aplicar funÃ§Ã£o para criar coluna com datas ajustadas\n",
    "profile['data_ajustada'] = profile['registered_on'].apply(ajustar_data)\n",
    "\n",
    "# Calcular tempo de casa em meses, do registro atÃ© hoje\n",
    "hoje = datetime.now()\n",
    "profile['tempo_de_casa'] = profile['data_ajustada'].apply(lambda x: meses_de_diferenca(x, hoje))\n",
    "\n",
    "# ğŸ” Visualizar resultado parcial\n",
    "print(profile[['registered_on', 'data_ajustada', 'tempo_de_casa']].head())\n",
    "\n",
    "\n",
    "\n",
    "# --- 8. Selecionar transaÃ§Ãµes e eventos relevantes\n",
    "\n",
    "# TransaÃ§Ãµes (compras feitas)\n",
    "transacoes = transactions_v2.query(\"event == 'transaction'\")[['account_id', 'time_since_test_start', 'amount']].copy()\n",
    "\n",
    "# Ofertas recebidas\n",
    "ofertas_recebidas = transactions_v2.query(\"event == 'offer received'\")[['account_id', 'offer_id', 'time_since_test_start']].copy()\n",
    "\n",
    "# Ofertas completadas\n",
    "ofertas_completas = transactions_v2.query(\"event == 'offer completed'\")[['account_id', 'offer_id', 'time_since_test_start']].copy()\n",
    "\n",
    "\n",
    "\n",
    "# --- 10. Relacionar transaÃ§Ãµes com as ofertas recebidas\n",
    "# --- 10. Relacionar transaÃ§Ãµes com as ofertas recebidas e verificar se foram utilizadas\n",
    "\n",
    "# 10.1: Filtrar apenas clientes que receberam ofertas\n",
    "clientes_com_oferta = ofertas_recebidas['account_id'].unique()\n",
    "\n",
    "# 10.2: Filtrar transaÃ§Ãµes apenas desses clientes\n",
    "transacoes_com_oferta = transacoes[transacoes['account_id'].isin(clientes_com_oferta)].copy()\n",
    "\n",
    "# 10.3: Adicionar duraÃ§Ã£o da oferta recebida\n",
    "ofertas_recebidas = ofertas_recebidas.merge(\n",
    "    offers[['offer_id', 'duration']],\n",
    "    on='offer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 10.4: Renomear e calcular validade da oferta\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "\n",
    "# 10.5: Juntar transaÃ§Ãµes com as ofertas recebidas por cliente\n",
    "trans_com_oferta = transacoes_com_oferta.merge(\n",
    "    ofertas_recebidas,\n",
    "    on='account_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 10.6: Manter apenas transaÃ§Ãµes dentro do perÃ­odo de validade da oferta\n",
    "trans_com_oferta = trans_com_oferta[\n",
    "    (trans_com_oferta['time_since_test_start'] >= trans_com_oferta['offer_start']) &\n",
    "    (trans_com_oferta['time_since_test_start'] <= trans_com_oferta['offer_end'])\n",
    "]\n",
    "\n",
    "# 10.7: Renomear coluna de conclusÃ£o da oferta\n",
    "ofertas_completas.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "# 10.8: Verificar se a transaÃ§Ã£o estÃ¡ associada a uma oferta completada\n",
    "trans_com_oferta = trans_com_oferta.merge(\n",
    "    ofertas_completas,\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 10.9: Criar coluna indicando se a oferta foi utilizada (completada apÃ³s a transaÃ§Ã£o)\n",
    "trans_com_oferta['oferta_utilizada'] = (\n",
    "    trans_com_oferta['completion_time'].notna() &\n",
    "    (trans_com_oferta['time_since_test_start'] <= trans_com_oferta['completion_time'])\n",
    ")\n",
    "\n",
    "# 10.10: Preencher valores nulos com False (nÃ£o completadas)\n",
    "trans_com_oferta['oferta_utilizada'] = trans_com_oferta['oferta_utilizada'].fillna(False)\n",
    "\n",
    "# 10.11: Selecionar colunas finais\n",
    "transacoes_final = trans_com_oferta[[\n",
    "    'account_id',\n",
    "    'time_since_test_start',\n",
    "    'amount',\n",
    "    'offer_id',\n",
    "    'oferta_utilizada'\n",
    "]].copy()\n",
    "\n",
    "# --- 15. Cruzar com dados do perfil dos usuÃ¡rios\n",
    "\n",
    "# Juntar com perfil do cliente\n",
    "base_final_filtrada = transacoes_final.merge(\n",
    "    profile,\n",
    "    on='account_id',\n",
    "    how='left'  # Use 'inner' para manter apenas quem tem perfil\n",
    ")\n",
    "\n",
    "base_final_filtrada.to_csv(r'C:\\Users\\F8090385\\ifood\\processed\\data_processed.csv', index = False, sep = ';' )\n",
    "# Exemplo de visualizaÃ§Ã£o final\n",
    "#print(base_final_filtrada.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06629130-ecd4-4a49-b2b2-3815b0c01eb7",
   "metadata": {},
   "source": [
    "## AnÃ¡lise ExploratÃ³ria de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a1647-3c21-4b0a-88cc-14587a9054af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas categÃ³ricas (exemplo)\n",
    "categorical_cols = ['oferta_utilizada', 'gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"Coluna: {col}\")\n",
    "    print(base_final_filtrada[col].value_counts(dropna=False))\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb90209-b7b4-4224-ab74-2b9f81accc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PRECISÃƒO ATUAL:', len(base_final_filtrada[base_final_filtrada['oferta_utilizada']==True])/len(base_final_filtrada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23378ad-1b4a-4922-9341-114478859f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['time_since_test_start', 'amount', 'age', 'credit_card_limit', 'tempo_de_casa']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    sns.histplot(base_final_filtrada[col].dropna(), kde=True)\n",
    "    plt.title(f'DistribuiÃ§Ã£o de {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a48dc5-0f57-495f-9100-518c2ba8701c",
   "metadata": {},
   "source": [
    "## Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24f142-42a8-4361-9dee-70d021ba5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofertas_recebidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978c888-3075-48d2-ab4e-b3d9a93c3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 17. Criando novas variÃ¡veis a partir da mÃ©dia da transaÃ§Ã£o e desconto.\n",
    "\n",
    "# Primeiro, garantir que 'offer_start' estÃ¡ no dataframe de ofertas_recebidas\n",
    "# JÃ¡ tem em seu cÃ³digo: ofertas_recebidas['offer_start']\n",
    "# 1. Expandir ofertas_recebidas para repetir para cada transaÃ§Ã£o do cliente\n",
    "transacoes_expand = transactions_v2[['account_id', 'time_since_test_start', 'amount', 'reward']].copy()\n",
    "\n",
    "# 2. Fazer merge para juntar transaÃ§Ãµes com ofertas por account_id\n",
    "merged = transacoes_expand.merge(\n",
    "    ofertas_recebidas[['account_id', 'offer_id', 'offer_start']],\n",
    "    on='account_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 3. Filtrar transaÃ§Ãµes que aconteceram antes (ou no dia) da oferta\n",
    "mask = merged['time_since_test_start'] <= merged['offer_start']\n",
    "merged_filtrado = merged[mask]\n",
    "\n",
    "# 4. Calcular mÃ©dia por account_id e offer_id\n",
    "medias = merged_filtrado.groupby(['account_id', 'offer_id'])[['amount', 'reward']].mean().reset_index()\n",
    "\n",
    "# 5. Renomear colunas para nÃ£o confundir\n",
    "medias.rename(columns={\n",
    "    'amount': 'mean_amount_ate_oferta',\n",
    "    'reward': 'mean_reward_ate_oferta'\n",
    "}, inplace=True)\n",
    "\n",
    "# 6. Juntar as mÃ©dias no dataframe original de ofertas_recebidas\n",
    "ofertas_recebidas = ofertas_recebidas.merge(\n",
    "    medias,\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 7. Agora junte essa informaÃ§Ã£o ao base_final_filtrada conforme vocÃª jÃ¡ fazia\n",
    "base_final_com_media = base_final_filtrada.merge(\n",
    "    ofertas_recebidas[['account_id', 'offer_id', 'mean_amount_ate_oferta', 'mean_reward_ate_oferta']],\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 18. Aplica a mÃ©dia em variÃ¡veis nÃºmericas que possuem nan.\n",
    "\n",
    "# Selecionar as colunas numÃ©ricas\n",
    "numericas = base_final_com_media.select_dtypes(include=['number']).columns\n",
    "\n",
    "#Limitar a idade, 118 Ã© bem estranho.\n",
    "#base_final_com_media['age'] = base_final_com_media['age'].clip(upper=100)\n",
    "\n",
    "# Preencher os NaNs com a mÃ©dia de cada coluna\n",
    "for col in numericas:\n",
    "    media_col = base_final_com_media[col].mean()\n",
    "    base_final_com_media[col].fillna(media_col, inplace=True)\n",
    "\n",
    "\n",
    "# --- 19. Criando novas variÃ¡veis \n",
    "\n",
    "# Usar o DataFrame que contÃ©m as mÃ©dias por cliente\n",
    "df = base_final_com_media.copy()\n",
    "\n",
    "df['gender'] = df['gender'].fillna('O')\n",
    "\n",
    "# Substituir zeros por NaN para evitar divisÃ£o por zero\n",
    "df['tempo_de_casa'] = df['tempo_de_casa'].replace(0, np.nan)\n",
    "df['mean_amount_ate_oferta'] = df['mean_amount_ate_oferta'].replace(0, np.nan)\n",
    "df['mean_reward_ate_oferta'] = df['mean_reward_ate_oferta'].replace(0, np.nan)\n",
    "\n",
    "# 1. 'tempo_desde_registro':  \n",
    "#    DiferenÃ§a entre o tempo da transaÃ§Ã£o (em dias desde o inÃ­cio do teste) \n",
    "#    e o tempo que o cliente estÃ¡ registrado na base (em dias).  \n",
    "#    Indica quanto tempo se passou desde o registro do cliente atÃ© a transaÃ§Ã£o.\n",
    "df['tempo_desde_registro'] = df['time_since_test_start'] - df['tempo_de_casa'] * 30\n",
    "\n",
    "# 2. 'amount_por_tempo_de_casa':  \n",
    "#    Gasto mÃ©dio mensal do cliente. Calculado dividindo o gasto mÃ©dio por transaÃ§Ã£o ('mean_amount') \n",
    "#    pelo tempo de casa (em meses).  \n",
    "#    Indica o quanto o cliente costuma gastar por mÃªs, em mÃ©dia.\n",
    "df['amount_por_tempo_de_casa'] = df['mean_amount_ate_oferta'] / df['tempo_de_casa']\n",
    "\n",
    "# 3. 'reward_por_amount':  \n",
    "#    EficiÃªncia da oferta para o cliente. Calculado pela mÃ©dia de recompensas ('mean_reward') \n",
    "#    dividida pela mÃ©dia de gastos ('mean_amount').  \n",
    "#    Indica quanto de recompensa o cliente gera proporcionalmente ao valor gasto.\n",
    "df['reward_por_amount'] = df['mean_reward_ate_oferta'] / df['mean_amount_ate_oferta']\n",
    "\n",
    "# 5. 'faixa_etaria':  \n",
    "#    Faixa etÃ¡ria categorizada do cliente, para segmentaÃ§Ã£o mais clara.  \n",
    "#    Categoriza a idade em intervalos predefinidos.\n",
    "# Calcular os quartis (incluindo mÃ­nimo e mÃ¡ximo)\n",
    "quartis = df['age'].quantile([0, 0.25, 0.5, 0.75, 1]).values\n",
    "\n",
    "# Criar labels dinÃ¢micas com base nos valores dos quartis\n",
    "labels = [f'{int(quartis[i])}-{int(quartis[i+1]-1)}' for i in range(len(quartis)-1)]\n",
    "\n",
    "# Criar a faixa etÃ¡ria usando os quartis e os labels gerados\n",
    "df['faixa_etaria'] = pd.cut(df['age'], bins=quartis, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eabba3-9b5f-4fc0-915b-bcd70fe472cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='faixa_etaria', hue='gender')\n",
    "plt.title('Contagem por faixa_etaria e gÃªnero')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=df[df['age']<100], x='faixa_etaria', hue='gender')\n",
    "plt.title('Contagem por faixa_etaria e gÃªnero')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526300f2-544b-4a7b-ba8a-ce781973c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dois comportamentos estranho genero O muito concetrado acima de 117. AlÃ©m disso, muito fora da curva ter muitos clientes com mais de 117 anos. Diante disso, esses dados serÃ£o excluÃ­dos.\n",
    "df = df[df['age'] <= 100]\n",
    "\n",
    "#recalcula as faixas:\n",
    "\n",
    "# Calcular os quartis (incluindo mÃ­nimo e mÃ¡ximo)\n",
    "quartis = df['age'].quantile([0, 0.25, 0.5, 0.75, 1]).values\n",
    "\n",
    "# Criar labels dinÃ¢micas com base nos valores dos quartis\n",
    "labels = [f'{int(quartis[i])}-{int(quartis[i+1]-1)}' for i in range(len(quartis)-1)]\n",
    "\n",
    "# Criar a faixa etÃ¡ria usando os quartis e os labels gerados\n",
    "df['faixa_etaria'] = pd.cut(df['age'], bins=quartis, labels=labels, include_lowest=True)\n",
    "\n",
    "sns.countplot(data=df, x='faixa_etaria', hue='gender')\n",
    "plt.title('Contagem por faixa_etaria e gÃªnero')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157b6ad-3cb9-49d6-8972-567cd9555c2d",
   "metadata": {},
   "source": [
    "## CorrelaÃ§Ã£o de VariÃ¡veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62797e-cd8f-46cd-9c1d-31e98a5cbe4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variaveis_numericas = df.select_dtypes(include='number').columns\n",
    "\n",
    "# Remover 'amount' e 'time_since_test_start' da lista, variaveis apÃ³s refletem diretamente o evento\n",
    "variaveis_numericas = [col for col in variaveis_numericas if col not in ['amount', 'time_since_test_start', 'tempo_desde_registro', 'amount_por_tempo_de_casa']]\n",
    "\n",
    "# CorrelaÃ§Ã£o Bi-variada\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(df[variaveis_numericas].corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Mapa de correlaÃ§Ã£o das variÃ¡veis numÃ©ricas')\n",
    "plt.show()\n",
    "\n",
    "# Calcular matriz de correlaÃ§Ã£o\n",
    "corr = df[variaveis_numericas].corr()\n",
    "\n",
    "# Mostrar a matriz com valores numÃ©ricos\n",
    "print(corr.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5a4d0-6953-4192-8cba-5f13161f79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CorrelaÃ§Ã£o com variÃ¡vel resposta\n",
    "\n",
    "# Calcular ANOVA para cada variÃ¡vel numÃ©rica vs offer_id\n",
    "resultados_anova = {}\n",
    "for var in variaveis_numericas:\n",
    "    grupos = [grupo[var].dropna() for _, grupo in df.groupby('offer_id')]\n",
    "    if all(len(grupo) > 1 for grupo in grupos):  # precisa de mais de 1 valor por grupo\n",
    "        stat, p = f_oneway(*grupos)\n",
    "        resultados_anova[var] = p\n",
    "\n",
    "# Mostrar ordenado por p-valor (quanto menor, mais significativa a diferenÃ§a entre grupos)\n",
    "pd.Series(resultados_anova).sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9ea1b-bbfd-432d-bff6-d95c42108fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variaveis_numericas:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=df, x='offer_id', y=var)\n",
    "    plt.title(f'{var} por offer_id')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08e78b-16ca-414c-97f3-8dc76aee4f6a",
   "metadata": {},
   "source": [
    "## Treinamento dos Resultados e AvaliaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac03da-95c2-4ad8-a607-a3aef0d97241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PrÃ©-processamento e divisÃ£o ---\n",
    "\n",
    "# Supondo que seu dataframe principal seja df, com target 'offer_id' e num_telefone em X (se nÃ£o, ajuste)\n",
    "\n",
    "# 2. Tratar target 'offer_id' com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['offer_id_encoded'] = le.fit_transform(df['offer_id'].astype(str))\n",
    "\n",
    "\n",
    "# OneHotEncoder para gender\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "gender_encoded = ohe.fit_transform(df[['gender']])\n",
    "\n",
    "# Extraindo categorias Ãºnicas de faixa_etaria\n",
    "unique_categories = list(df['faixa_etaria'].unique())\n",
    "\n",
    "\n",
    "\n",
    "# Nomes das colunas para gender_encoded\n",
    "gender_encoded_cols = ohe.get_feature_names_out(['gender'])\n",
    "\n",
    "# Criar df para codificaÃ§Ãµes\n",
    "df_gender_encoded = pd.DataFrame(gender_encoded, columns=gender_encoded_cols, index=df.index)\n",
    "\n",
    "# Concatenar com as numÃ©ricas\n",
    "X = pd.concat([df[variaveis_numericas], df_gender_encoded], axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['offer_id_encoded']\n",
    "\n",
    "# Selecionar features e target\n",
    "#X = df[variaveis_numericas]\n",
    "#y = df['offer_id_encoded']\n",
    "\n",
    "# Dividir em treino e teste (por exemplo, 80% treino, 20% teste)\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Tamanho treino: {X_train_clean.shape[0]}')\n",
    "print(f'Tamanho teste: {X_test_clean.shape[0]}')\n",
    "\n",
    "# 5. Preparar dataframe vazio para mÃ©tricas\n",
    "df_metricas_por_grupo = pd.DataFrame()\n",
    "\n",
    "# 6. Chamada da funÃ§Ã£o\n",
    "df_metricas_por_grupo = aprendizado_supervisionado(\n",
    "    X_train_clean,\n",
    "    X_test_clean,\n",
    "    y_train_clean,\n",
    "    y_test_clean,\n",
    "    df_metricas_por_grupo,\n",
    "    X_test_clean\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd686a8-8e22-4b23-916a-01bcb36546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# 5 aqui corresponde ao ni\n",
    "print(df[['offer_id', 'offer_id_encoded']].drop_duplicates( keep = 'last'))\n",
    "\n",
    "X_cluster = X_train_clean[y_train_clean!= 5].copy()\n",
    "X_cluster = X_cluster.reset_index(drop=True)\n",
    "\n",
    "modelo, scaler, centroides, raio_max, resumo = treinar_kmeans(\n",
    "    produto='ifood_oferta',\n",
    "    X_train=X_cluster[variaveis_numericas]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eddf77-1ddd-4fb0-b94c-59e926d83942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['offer_id', 'offer_id_encoded']].drop_duplicates( keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e16244-a11c-45f3-b05b-6ae2a5709960",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41803af-3ac1-4596-861e-8df3cb0975eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'age',\n",
    " 'credit_card_limit',\n",
    " 'tempo_de_casa',\n",
    " 'mean_amount',\n",
    " 'mean_reward',\n",
    " 'tempo_desde_registro',\n",
    " 'amount_por_tempo_de_casa',\n",
    " 'reward_por_amount, \n",
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5d949-48cc-4a7a-a06c-03cc16ad4e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb2e27-3945-477c-b771-cd0c2e76ab4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405a18-0a2f-482b-ad75-5a2bac8ab1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cfc0f-43ac-42cd-8286-1759fde91b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar features e target\n",
    "X = df[variaveis_numericas]\n",
    "y = df['offer_id']\n",
    "\n",
    "# Dividir em treino e teste (por exemplo, 80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Tamanho treino: {X_train.shape[0]}')\n",
    "print(f'Tamanho teste: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e6286-3ac7-4824-b87d-c70a9935d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# --- FunÃ§Ã£o de aprendizado supervisionado ajustada para multi-classes (offer_id) ---\n",
    "def aprendizado_supervisionado(X_train, X_test, y_train, y_test, df_metricas_por_grupo, X_test_geral):\n",
    "    modelos = {\n",
    "        'LogisticRegression': (\n",
    "            LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs'),\n",
    "            {'clf__C': [0.1, 1, 10]}\n",
    "        ),\n",
    "        'DecisionTree': (\n",
    "            DecisionTreeClassifier(random_state=42),\n",
    "            {'clf__max_depth': [None, 5, 10, 20], 'clf__min_samples_split': [2, 5, 10]}\n",
    "        ),\n",
    "        'RandomForest': (\n",
    "            RandomForestClassifier(random_state=42),\n",
    "            {'clf__n_estimators': [100, 200], 'clf__max_depth': [None, 5, 10]}\n",
    "        ),\n",
    "        'XGBClassifier': (\n",
    "            XGBClassifier(eval_metric='mlogloss', tree_method='hist', random_state=42),\n",
    "            {'clf__n_estimators': [100, 200], 'clf__max_depth': [3, 6], 'clf__learning_rate': [0.05, 0.1]}\n",
    "        )\n",
    "    }\n",
    "\n",
    "    melhores_modelos = {}\n",
    "    lista_metricas_por_grupo = []\n",
    "    \n",
    "    # DataFrame para salvar probabilidades (probabilidades para cada classe)\n",
    "    df_probabilidades = pd.DataFrame()\n",
    "    if 'num_telefone' in X_test_geral.columns:\n",
    "        df_probabilidades['num_telefone'] = X_test_geral['num_telefone']\n",
    "    df_probabilidades['true_offer_id'] = y_test.reset_index(drop=True)\n",
    "\n",
    "    for nome, (modelo, parametros) in modelos.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', modelo)\n",
    "        ])\n",
    "\n",
    "        grid = GridSearchCV(pipeline, param_grid=parametros, scoring='f1_weighted', cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        melhores_modelos[nome] = grid.best_estimator_\n",
    "\n",
    "        # Prever classes e probabilidades\n",
    "        y_pred = grid.predict(X_test)\n",
    "        y_pred_proba = grid.predict_proba(X_test)\n",
    "\n",
    "        # Adicionar colunas de probabilidade para cada classe no df_probabilidades\n",
    "        for idx, classe in enumerate(grid.classes_):\n",
    "            col_name = f'proba_{nome}_{classe}'\n",
    "            df_probabilidades[col_name] = y_pred_proba[:, idx]\n",
    "\n",
    "        # MÃ©tricas multi-classe\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f'ALGORITMO: {nome} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}')\n",
    "\n",
    "        lista_metricas_por_grupo.append({\n",
    "            'metodo': f'SUP. MULTI ({nome})',\n",
    "            'precision': round(precision, 4),\n",
    "            'recall': round(recall, 4),\n",
    "            'f1': round(f1, 4)\n",
    "        })\n",
    "\n",
    "    # Salvar probabilidades\n",
    "    df_probabilidades.to_csv('probabilidades_por_algoritmo_multiclasse.csv', index=False, sep=';')\n",
    "    print(f\"ğŸ’¾ Probabilidades salvas em: bases/probabilidades_por_algoritmo_multiclasse.csv\")\n",
    "\n",
    "    # Salvar o melhor modelo baseado na F1 score\n",
    "    melhor_modelo = max(lista_metricas_por_grupo, key=lambda x: x['f1'])\n",
    "    nome_melhor = melhor_modelo['metodo'].split('(')[-1].replace(')', '')\n",
    "    modelo_para_salvar = melhores_modelos[nome_melhor]\n",
    "    joblib.dump(modelo_para_salvar, 'melhor_modelo_f1_multiclasse.pkl')\n",
    "\n",
    "    # Concatenar mÃ©tricas ao df original\n",
    "    df_novos = pd.DataFrame(lista_metricas_por_grupo)\n",
    "    df_metricas_por_grupo = pd.concat([df_metricas_por_grupo, df_novos], ignore_index=True)\n",
    "        \n",
    "    print(f\"\\nâœ… Melhor modelo salvo: {nome_melhor} com F1-score {melhor_modelo['f1']:.4f}\")\n",
    "    return df_metricas_por_grupo\n",
    "\n",
    "\n",
    "# --- PrÃ©-processamento e divisÃ£o ---\n",
    "\n",
    "# Supondo que seu dataframe principal seja df, com target 'offer_id' e num_telefone em X (se nÃ£o, ajuste)\n",
    "\n",
    "# 2. Tratar target 'offer_id' com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['offer_id_encoded'] = le.fit_transform(df['offer_id'].astype(str))\n",
    "\n",
    "# Selecionar features e target\n",
    "X = df[variaveis_numericas]\n",
    "y = df['offer_id_encoded']\n",
    "\n",
    "\n",
    "print(f'Tamanho treino: {X_train.shape[0]}')\n",
    "print(f'Tamanho teste: {X_test.shape[0]}')\n",
    "\n",
    "# Dividir em treino e teste (por exemplo, 80% treino, 20% teste)\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Preparar dataframe vazio para mÃ©tricas\n",
    "df_metricas_por_grupo = pd.DataFrame()\n",
    "\n",
    "# 6. Chamada da funÃ§Ã£o\n",
    "df_metricas_por_grupo = aprendizado_supervisionado(\n",
    "    X_train_clean[variaveis_numericas],\n",
    "    X_test_clean[variaveis_numericas],\n",
    "    y_train_clean,\n",
    "    y_test_clean,\n",
    "    df_metricas_por_grupo,\n",
    "    X_test_clean\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf85a7d-f1b2-44ba-b2ac-c83323fb7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76a1c-56fe-440d-b9c3-f6440cb4b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_v2.columns - Index(['event', 'account_id', 'time_since_test_start', 'amount', 'offer_id',\n",
    "       'reward'],\n",
    "\n",
    "verificar se todas offer completed possuem transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee1200-300e-4da7-a1fd-7ca67786a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Filtrar eventos principais\n",
    "transacoes = transactions_v2[transactions_v2['event'] == 'transaction'][['account_id', 'time_since_test_start', 'amount']].copy()\n",
    "\n",
    "ofertas_recebidas = transactions_v2[transactions_v2['event'] == 'offer received'][['account_id', 'offer_id', 'time_since_test_start']].copy()\n",
    "\n",
    "# Juntar 'ofertas_recebidas' com 'offers' usando a chave 'offer_id'\n",
    "ofertas_recebidas = pd.merge(\n",
    "    ofertas_recebidas,\n",
    "    offers[['offer_id', 'duration']],\n",
    "    on='offer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "ofertas_completas = transactions_v2[transactions_v2['event'] == 'offer completed'][['account_id', 'offer_id', 'time_since_test_start']].copy()\n",
    "\n",
    "# 2. Renomear colunas para clareza\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "ofertas_completas.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "# 3. Juntar transaÃ§Ãµes com as ofertas recebidas do mesmo cliente\n",
    "trans_com_oferta = transacoes.merge(ofertas_recebidas, on='account_id', how='left')\n",
    "\n",
    "# 4. Manter apenas transaÃ§Ãµes dentro da janela da oferta\n",
    "trans_com_oferta = trans_com_oferta[\n",
    "    (trans_com_oferta['time_since_test_start'] >= trans_com_oferta['offer_start']) &\n",
    "    (trans_com_oferta['time_since_test_start'] <= trans_com_oferta['offer_end'])\n",
    "]\n",
    "\n",
    "# 5. Juntar com ofertas completadas para saber se contribuÃ­ram\n",
    "trans_com_oferta = trans_com_oferta.merge(\n",
    "    ofertas_completas,\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 6. Marcar se a transaÃ§Ã£o ocorreu antes ou igual Ã  conclusÃ£o\n",
    "trans_com_oferta['contribuiu_para_oferta'] = (\n",
    "    trans_com_oferta['time_since_test_start'] <= trans_com_oferta['completion_time']\n",
    ").fillna(False)\n",
    "\n",
    "# 7. Selecionar apenas transaÃ§Ãµes que realmente contribuÃ­ram\n",
    "trans_contribuintes = trans_com_oferta[trans_com_oferta['contribuiu_para_oferta']].copy()\n",
    "\n",
    "# 8. Selecionar colunas de interesse\n",
    "trans_contribuintes = trans_contribuintes[['account_id', 'time_since_test_start', 'amount', 'offer_id', 'contribuiu_para_oferta']]\n",
    "\n",
    "# 9. Unir com todas as transaÃ§Ãµes originais (incluir tambÃ©m as que NÃƒO contribuÃ­ram)\n",
    "transacoes_final = transacoes.merge(\n",
    "    trans_contribuintes,\n",
    "    on=['account_id', 'time_since_test_start', 'amount'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 10. Preencher valores nulos\n",
    "transacoes_final['contribuiu_para_oferta'] = transacoes_final['contribuiu_para_oferta'].fillna(False)\n",
    "transacoes_final['offer_id'] = transacoes_final['offer_id'].fillna(pd.NA)\n",
    "\n",
    "# âœ… Resultado: todas as transaÃ§Ãµes com info se contribuÃ­ram ou nÃ£o\n",
    "print(transacoes_final.head())\n",
    "\n",
    "# Para cada transaÃ§Ã£o, pegar a oferta completada (se houver)\n",
    "trans_com_oferta = transacoes_final.copy()\n",
    "\n",
    "# Filtrar apenas as transaÃ§Ãµes que contribuÃ­ram para alguma oferta\n",
    "trans_com_oferta_valid = trans_com_oferta[trans_com_oferta['contribuiu_para_oferta']]\n",
    "\n",
    "# Escolher a oferta que mais contribuiu por transaÃ§Ã£o (exemplo: primeira ou maior reward)\n",
    "# Se quiser, pode agrupar por (account_id, time_since_test_start) e escolher a oferta\n",
    "\n",
    "melhores_ofertas = trans_com_oferta_valid.groupby(\n",
    "    ['account_id', 'time_since_test_start', 'amount']\n",
    ")['offer_id'].first().reset_index()\n",
    "\n",
    "# Juntar com a base original para ter todas as transaÃ§Ãµes\n",
    "base_modelo = transacoes.merge(\n",
    "    melhores_ofertas,\n",
    "    on=['account_id', 'time_since_test_start', 'amount'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Marca se houve ou nÃ£o oferta\n",
    "base_modelo['teve_oferta'] = base_modelo['offer_id'].notna()\n",
    "\n",
    "print(base_modelo.head())\n",
    "\n",
    "base_modelo.loc[base_modelo['offer_id'].isna(), 'offer_id'] = 'NI'\n",
    "\n",
    "# Cruzar profile com base_modelo usando account_id\n",
    "base_final = pd.merge(\n",
    "    base_modelo,\n",
    "    profile,\n",
    "    on='account_id',\n",
    "    how='left'  # ou 'inner' se quiser manter apenas os que tÃªm perfil\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ada80-a971-41ec-b765-f4e33eb37884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba5c2e-86e0-493b-8caa-09723d330451",
   "metadata": {},
   "outputs": [],
   "source": [
    "InformaÃ§Ã£o de temporal de quando a transaÃ§Ã£o ocorreu, oq considerar?\n",
    "time_since_test_start (int): tempo desde o comeÃ§o do teste em dias (t=0), oq seria teste aqui?\n",
    "Estou assumindo a premissa de que dada todas as transaÃ§Ãµes quais foram tiveram ou ofertas, alÃ©m disso, pode ser que um determinada oferta possa sido recebida e usada mais de uma vez\n",
    "EstÃ¡ correta a premissa abaixo: \n",
    "\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "ofertas_completas.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "trans_com_oferta = trans_com_oferta[\n",
    "    (trans_com_oferta['time_since_test_start'] >= trans_com_oferta['offer_start']) &\n",
    "    (trans_com_oferta['time_since_test_start'] <= trans_com_oferta['offer_end'])\n",
    "]\n",
    "\n",
    "Estou assumindo premissas e gasntando mais tempo com a engenharia de dados por nÃ£o conhecer a fundo o informacional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cbf20-0406-47bd-bad5-75136d97ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_modelo.groupby(['offer_id']).count()\n",
    "\n",
    "base_modelo[base_modelo['offer_id'] != 'NI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8842063-316c-4103-8e62-bba07c022be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Todas as transaÃ§Ãµes\n",
    "transacoes = transactions_v2[transactions_v2['event'] == 'transaction'][['account_id', 'time_since_test_start', 'amount', 'offer_id']].copy()\n",
    "\n",
    "# 2. Ofertas recebidas (tempo inÃ­cio e duraÃ§Ã£o)\n",
    "ofertas_recebidas = transactions_v2[transactions_v2['event'] == 'offer received'][['account_id', 'offer_id', 'time_since_test_start']].copy()\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "\n",
    "# Importante: pegar duraÃ§Ã£o da oferta no df offers e juntar\n",
    "ofertas_recebidas = ofertas_recebidas.merge(\n",
    "    offers[['offer_id', 'duration']],\n",
    "    on='offer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "ofertas_completas = transactions_v2[transactions_v2['event'] == 'offer completed'][['account_id', 'offer_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f811504-0939-4332-af40-bebee44cb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb377d63-c9b3-49e7-ada3-4c7f96f10f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Ofertas completadas \n",
    "ofertas_completas = transactions_v2[transactions_v2['event'] == 'offer completed'][['account_id', 'offer_id']]\n",
    "\n",
    "# 4. Juntar transaÃ§Ãµes com ofertas recebidas pelo cliente e offer_id para pegar janela vÃ¡lida\n",
    "transacoes_com_oferta = transacoes.merge(\n",
    "    ofertas_recebidas,\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5. Filtrar transaÃ§Ãµes que estÃ£o dentro da janela da oferta\n",
    "transacoes_com_oferta = transacoes_com_oferta[\n",
    "    (transacoes_com_oferta['time_since_test_start'] >= transacoes_com_oferta['offer_start']) &\n",
    "    (transacoes_com_oferta['time_since_test_start'] <= transacoes_com_oferta['offer_end'])\n",
    "]\n",
    "\n",
    "# 6. Marcar se a oferta foi completada\n",
    "transacoes_com_oferta = transacoes_com_oferta.merge(\n",
    "    ofertas_completas.assign(oferta_completa=True),\n",
    "    on=['account_id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "transacoes_com_oferta['oferta_completa'] = transacoes_com_oferta['oferta_completa'].fillna(False)\n",
    "\n",
    "# 7. TransaÃ§Ãµes fora do perÃ­odo da oferta: nÃ£o tem oferta completa\n",
    "transacoes_sem_oferta = transacoes.merge(\n",
    "    transacoes_com_oferta[['account_id', 'time_since_test_start', 'offer_id']],\n",
    "    on=['account_id', 'time_since_test_start', 'offer_id'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# TransaÃ§Ãµes que nÃ£o entraram na janela da oferta\n",
    "transacoes_fora_janela = transacoes_sem_oferta[transacoes_sem_oferta['_merge'] == 'left_only'].copy()\n",
    "transacoes_fora_janela['oferta_completa'] = False\n",
    "\n",
    "# 8. Juntar tudo (dentro da janela + fora da janela)\n",
    "resultado_final = pd.concat([transacoes_com_oferta, transacoes_fora_janela[transacoes.columns.tolist() + ['oferta_completa']]], ignore_index=True)\n",
    "\n",
    "print(resultado_final.head())\n",
    "print(f\"Total transaÃ§Ãµes: {len(resultado_final)}\")\n",
    "print(f\"TransaÃ§Ãµes com oferta completa: {resultado_final['oferta_completa'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1eb06-4031-4ad2-878a-ba0a8e699782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TransaÃ§Ãµes totais: {len(transacoes)}\")\n",
    "print(f\"Ofertas recebidas: {len(ofertas_recebidas)}\")\n",
    "print(f\"Ofertas completas: {len(ofertas_completas)}\")\n",
    "\n",
    "print(\"\\nExemplo ofertas recebidas:\")\n",
    "print(ofertas_recebidas.head())\n",
    "\n",
    "print(\"\\nExemplo transaÃ§Ãµes com oferta apÃ³s merge:\")\n",
    "print(transacoes_com_oferta.head())\n",
    "\n",
    "print(transactions_v2.groupby(['event']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d7038-4218-4fc8-8e8b-9fa30abc62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Filtrar eventos de \"offer received\" com os dados relevantes\n",
    "offers = base_treino[base_treino['event'] == 'offer received'][['id', 'offer_id', 'time_since_test_start', 'duration']].copy()\n",
    "offers.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "\n",
    "# 2. Calcular o fim da validade da oferta\n",
    "offers['offer_end'] = offers['offer_start'] + offers['duration']\n",
    "\n",
    "# 3. Filtrar eventos de \"offer completed\"\n",
    "completions = base_treino[base_treino['event'] == 'offer completed'][['id', 'offer_id', 'time_since_test_start']].copy()\n",
    "completions.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "# 4. Merge: combinar ofertas recebidas com completadas (por id + offer_id)\n",
    "merged = pd.merge(offers, completions, on=['id', 'offer_id'], how='left')\n",
    "\n",
    "# 5. Verificar se completou dentro da validade\n",
    "merged['completed_on_time'] = (\n",
    "    (merged['completion_time'] >= merged['offer_start']) &\n",
    "    (merged['completion_time'] <= merged['offer_end'])\n",
    ").fillna(False).astype(int)\n",
    "\n",
    "# Agora vocÃª tem um dataframe com uma coluna binÃ¡ria indicando se completou a oferta no tempo:\n",
    "# 1 = completou dentro do prazo\n",
    "# 0 = nÃ£o completou\n",
    "\n",
    "# Exemplo de saÃ­da:\n",
    "print(merged[['id', 'offer_id', 'offer_start', 'offer_end', 'completion_time', 'completed_on_time']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847109c-0a50-42d6-98f1-bd97f9a38c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['completed_on_time']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e86d1-dda4-4f0c-a439-d8d20a3eed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. TransaÃ§Ãµes\n",
    "transacoes = base_treino[base_treino['event'] == 'transaction'].copy()\n",
    "transacoes = transacoes[['id', 'time_since_test_start', 'amount']]  # Adicione mais colunas se quiser\n",
    "\n",
    "# 2. Ofertas recebidas\n",
    "ofertas_recebidas = base_treino[base_treino['event'] == 'offer received'][['id', 'offer_id', 'time_since_test_start', 'duration']].copy()\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "\n",
    "# âœ… VerificaÃ§Ã£o extra para garantir que 'offer_id' estÃ¡ presente\n",
    "assert 'offer_id' in ofertas_recebidas.columns, \"Coluna 'offer_id' ausente em ofertas_recebidas\"\n",
    "\n",
    "# 3. Ofertas completadas\n",
    "ofertas_completas = base_treino[base_treino['event'] == 'offer completed'][['id', 'offer_id', 'time_since_test_start']].copy()\n",
    "ofertas_completas.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "# 4. Juntar transaÃ§Ãµes com as ofertas recebidas (mesmo cliente)\n",
    "trans_com_oferta = pd.merge(\n",
    "    transacoes,\n",
    "    ofertas_recebidas,\n",
    "    on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# âœ… Garantir que 'offer_id' estÃ¡ presente apÃ³s merge\n",
    "assert 'offer_id' in trans_com_oferta.columns, \"Coluna 'offer_id' ausente apÃ³s merge com ofertas_recebidas\"\n",
    "\n",
    "# 5. Manter transaÃ§Ãµes que ocorreram durante a validade da oferta\n",
    "trans_com_oferta = trans_com_oferta[\n",
    "    (trans_com_oferta['time_since_test_start'] >= trans_com_oferta['offer_start']) &\n",
    "    (trans_com_oferta['time_since_test_start'] <= trans_com_oferta['offer_end'])\n",
    "].copy()\n",
    "\n",
    "# 6. Juntar com ofertas completadas (id + offer_id)\n",
    "trans_com_oferta = pd.merge(\n",
    "    trans_com_oferta,\n",
    "    ofertas_completas,\n",
    "    on=['id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 7. Marcar se transaÃ§Ã£o contribuiu para a conclusÃ£o da oferta\n",
    "trans_com_oferta['transacao_contribuiu'] = (\n",
    "    trans_com_oferta['time_since_test_start'] <= trans_com_oferta['completion_time']\n",
    ").fillna(False).astype(int)\n",
    "\n",
    "# 8. Resultado\n",
    "print(trans_com_oferta[['id', 'offer_id', 'time_since_test_start', 'amount', 'completion_time', 'transacao_contribuiu']].head())\n",
    "print(f\"Total de transaÃ§Ãµes com match a ofertas vÃ¡lidas: {len(trans_com_oferta)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7ed19-5cdd-4cea-9307-b8bbc8dbe7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_com_oferta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b522d63-207e-4c5f-9b02-a11ee0af244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar todas as transaÃ§Ãµes\n",
    "transacoes = base_treino[base_treino['event'] == 'transaction'].copy()\n",
    "transacoes = transacoes[['id', 'time_since_test_start', 'amount']]\n",
    "\n",
    "# 2. Ofertas recebidas (com duraÃ§Ã£o)\n",
    "ofertas_recebidas = base_treino[base_treino['event'] == 'offer received'][['id', 'offer_id', 'time_since_test_start', 'duration']].copy()\n",
    "ofertas_recebidas.rename(columns={'time_since_test_start': 'offer_start'}, inplace=True)\n",
    "ofertas_recebidas['offer_end'] = ofertas_recebidas['offer_start'] + ofertas_recebidas['duration']\n",
    "\n",
    "# 3. Ofertas completadas\n",
    "ofertas_completas = base_treino[base_treino['event'] == 'offer completed'][['id', 'offer_id', 'time_since_test_start']].copy()\n",
    "ofertas_completas.rename(columns={'time_since_test_start': 'completion_time'}, inplace=True)\n",
    "\n",
    "# 4. Juntar transaÃ§Ãµes com as ofertas recebidas do mesmo cliente\n",
    "# --> Isso pode gerar vÃ¡rias linhas por transaÃ§Ã£o (uma para cada oferta recebida pelo cliente)\n",
    "transacoes_com_ofertas = pd.merge(transacoes, ofertas_recebidas, on='id', how='left')\n",
    "\n",
    "# 5. Marcar se a transaÃ§Ã£o caiu no perÃ­odo de alguma oferta (nova coluna)\n",
    "transacoes_com_ofertas['dentro_da_oferta'] = (\n",
    "    (transacoes_com_ofertas['time_since_test_start'] >= transacoes_com_ofertas['offer_start']) &\n",
    "    (transacoes_com_ofertas['time_since_test_start'] <= transacoes_com_ofertas['offer_end'])\n",
    ")\n",
    "\n",
    "# 6. Juntar com as ofertas completadas (id + offer_id)\n",
    "transacoes_com_ofertas = pd.merge(\n",
    "    transacoes_com_ofertas,\n",
    "    ofertas_completas,\n",
    "    on=['id', 'offer_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 7. Marcar se a transaÃ§Ã£o ocorreu antes da conclusÃ£o da oferta\n",
    "transacoes_com_ofertas['transacao_contribuiu'] = (\n",
    "    transacoes_com_ofertas['time_since_test_start'] <= transacoes_com_ofertas['completion_time']\n",
    ") & (transacoes_com_ofertas['dentro_da_oferta'])\n",
    "\n",
    "# 8. Substituir NaNs (casos sem match com oferta)\n",
    "transacoes_com_ofertas['transacao_contribuiu'] = transacoes_com_ofertas['transacao_contribuiu'].fillna(False).astype(int)\n",
    "\n",
    "# âœ… Resultado final com todas as transaÃ§Ãµes (inclusive sem oferta)\n",
    "print(transacoes_com_ofertas[['id', 'time_since_test_start', 'amount', 'offer_id', 'dentro_da_oferta', 'completion_time', 'transacao_contribuiu']].head())\n",
    "\n",
    "trans_com_oferta = transacoes_com_ofertas.merge(\n",
    "    base_treino[['id', 'time_since_test_start', 'age', 'gender', 'credit_card_limit']],  # adicione o que quiser\n",
    "    on=['id', 'time_since_test_start'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf5105-9ee4-46dc-a368-a050e29e5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_com_oferta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae7234-5c12-49b6-9aa7-042bddf63b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino.groupby(['event']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0d2d2-5bb1-4aea-9204-f9c7c3f52612",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968f309-635d-4af4-a4a0-ea8b2fb7e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#desconsiderar transaction\n",
    "\n",
    "base_treino = profile_v2.copy()\n",
    "\n",
    "#base_treino = profile_v2[profile_v2['event'] != 'transaction']\n",
    "\n",
    "#base_treino.loc[base_treino['event'] != 'offer completed', 'target'] = 0\n",
    "#base_treino.loc[base_treino['event'] == 'offer completed', 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d883140-5b83-4d44-aee7-082360b86666",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino.groupby(['event']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105be2f-f751-42ce-b895-f29b57be2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c487d6d-c8ef-444e-952d-9307e036d451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fc271-423a-40da-b413-70a19ca61e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Filtrar eventos de transaÃ§Ã£o\n",
    "transacoes = base_treino[base_treino['event'] == 'transaction'].copy()\n",
    "transacoes['amount'] = pd.to_numeric(transacoes['amount'], errors='coerce')\n",
    "transacoes = transacoes.dropna(subset=['amount'])\n",
    "\n",
    "# Remover possÃ­veis NaNs em time_since_test_start (chave)\n",
    "transacoes = transacoes.dropna(subset=['time_since_test_start'])\n",
    "\n",
    "# Ordenar e resetar Ã­ndice\n",
    "transacoes = transacoes.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "\n",
    "# Criar cumulativos\n",
    "transacoes['transaction_count_cum'] = 1\n",
    "transacoes['transaction_count_cum'] = transacoes.groupby('id')['transaction_count_cum'].cumsum()\n",
    "transacoes['total_transacted_cum'] = transacoes.groupby('id')['amount'].cumsum()\n",
    "transacoes['avg_transaction_cum'] = transacoes['total_transacted_cum'] / transacoes['transaction_count_cum']\n",
    "\n",
    "# 2. Filtrar eventos de oferta recebida ou completada\n",
    "base_modelo = base_treino[base_treino['event'].isin(['offer received', 'offer completed'])].copy()\n",
    "\n",
    "# Remover NaNs em time_since_test_start (chave) na base_modelo\n",
    "base_modelo = base_modelo.dropna(subset=['time_since_test_start'])\n",
    "\n",
    "# Ordenar e resetar Ã­ndice\n",
    "base_modelo = base_modelo.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "\n",
    "# 3. Merge asof â€” juntar cumulativos atÃ© momento da oferta\n",
    "merged = pd.merge_asof(\n",
    "    base_modelo,\n",
    "    transacoes[['id', 'time_since_test_start', 'transaction_count_cum', 'total_transacted_cum', 'avg_transaction_cum']],\n",
    "    on='time_since_test_start',\n",
    "    by='id',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# 4. Renomear colunas\n",
    "merged.rename(columns={\n",
    "    'transaction_count_cum': 'transaction_count_before',\n",
    "    'total_transacted_cum': 'total_transacted_before',\n",
    "    'avg_transaction_cum': 'avg_transaction_before'\n",
    "}, inplace=True)\n",
    "\n",
    "# 5. Preencher valores nulos\n",
    "merged[['transaction_count_before', 'total_transacted_before', 'avg_transaction_before']] = \\\n",
    "    merged[['transaction_count_before', 'total_transacted_before', 'avg_transaction_before']].fillna(0)\n",
    "\n",
    "# 6. Criar last_transaction_time: o tempo da Ãºltima transaÃ§Ã£o anterior Ã  oferta\n",
    "# Podemos fazer um merge separado para pegar o Ãºltimo tempo da transaÃ§Ã£o, mas uma soluÃ§Ã£o simples:\n",
    "# Usar merge_asof para capturar tempo da Ãºltima transaÃ§Ã£o\n",
    "last_trans_time = transacoes[['id', 'time_since_test_start']].rename(columns={'time_since_test_start': 'last_transaction_time'})\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    merged.sort_values(['id', 'time_since_test_start']),\n",
    "    last_trans_time.sort_values(['id', 'last_transaction_time']),\n",
    "    left_on='time_since_test_start',\n",
    "    right_on='last_transaction_time',\n",
    "    by='id',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Preencher NaNs de last_transaction_time com -1 (sem transaÃ§Ãµes anteriores)\n",
    "merged['last_transaction_time'] = merged['last_transaction_time'].fillna(-1)\n",
    "\n",
    "# 7. Base final pronta\n",
    "base_modelo = merged\n",
    "\n",
    "print(base_modelo[['id', 'offer_id', 'event', 'time_since_test_start', \n",
    "                  'transaction_count_before', 'total_transacted_before', \n",
    "                  'avg_transaction_before', 'last_transaction_time']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4f698-528c-4425-b867-4015386ed5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbcf0a-b4a8-4af0-bce4-5f5f53e8cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo fictÃ­cio de criaÃ§Ã£o dos DataFrames base_modelo e transacoes\n",
    "# (substitua pela sua leitura real)\n",
    "# base_modelo = pd.read_csv('base_modelo.csv')\n",
    "# transacoes = pd.read_csv('transacoes.csv')\n",
    "\n",
    "# --- Passo 1: ordenar ambos os DataFrames por ['id', 'time_since_test_start']\n",
    "base_modelo = base_modelo.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "transacoes = transacoes.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "\n",
    "# --- Passo 2: verificar duplicatas na chave que serÃ¡ usada no merge\n",
    "duplicatas_base = base_modelo.duplicated(subset=['id', 'time_since_test_start'], keep=False)\n",
    "duplicatas_trans = transacoes.duplicated(subset=['id', 'time_since_test_start'], keep=False)\n",
    "\n",
    "print(f\"Duplicatas em base_modelo na chave ['id', 'time_since_test_start']: {duplicatas_base.sum()}\")\n",
    "print(f\"Duplicatas em transacoes na chave ['id', 'time_since_test_start']: {duplicatas_trans.sum()}\")\n",
    "\n",
    "# --- Passo 3: se houver duplicatas, ajustar para evitar erro no merge_asof\n",
    "def ajustar_duplicatas(df):\n",
    "    # Para cada grupo id, adiciona um pequeno incremento cumulativo para garantir unicidade dos timestamps\n",
    "    def adjust_group(g):\n",
    "        # Identificar duplicados no grupo\n",
    "        dup_mask = g.duplicated(subset=['time_since_test_start'], keep=False)\n",
    "        if dup_mask.any():\n",
    "            # Para os duplicados, acrescentar um pequeno incremento incremental\n",
    "            increments = np.arange(len(g)) * 1e-9  # incremento minÃºsculo para ordenar sem alterar valores significativamente\n",
    "            g.loc[:, 'time_since_test_start'] = g['time_since_test_start'] + increments\n",
    "        return g\n",
    "\n",
    "    df = df.groupby('id', group_keys=False).apply(adjust_group)\n",
    "    return df\n",
    "\n",
    "if duplicatas_base.sum() > 0:\n",
    "    base_modelo = ajustar_duplicatas(base_modelo)\n",
    "if duplicatas_trans.sum() > 0:\n",
    "    transacoes = ajustar_duplicatas(transacoes)\n",
    "\n",
    "# --- Passo 4: ordenar novamente para garantir ordenaÃ§Ã£o apÃ³s ajuste\n",
    "base_modelo = base_modelo.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "transacoes = transacoes.sort_values(['id', 'time_since_test_start']).reset_index(drop=True)\n",
    "\n",
    "# --- Passo 5: verificar monotonicidade da coluna time_since_test_start (deve ser True)\n",
    "print(\"base_modelo time_since_test_start is monotonic increasing?\", base_modelo['time_since_test_start'].is_monotonic_increasing)\n",
    "print(\"transacoes time_since_test_start is monotonic increasing?\", transacoes['time_since_test_start'].is_monotonic_increasing)\n",
    "\n",
    "# --- Passo 6: executar merge_asof\n",
    "merged = pd.merge_asof(\n",
    "    base_modelo,\n",
    "    transacoes[['id', 'time_since_test_start', 'transaction_count_cum', 'total_transacted_cum', 'avg_transaction_cum']],\n",
    "    on='time_since_test_start',\n",
    "    by='id',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# --- Passo 7: renomear colunas cumulativas para nomes desejados\n",
    "merged.rename(columns={\n",
    "    'transaction_count_cum': 'transaction_count_before',\n",
    "    'total_transacted_cum': 'total_transacted_before',\n",
    "    'avg_transaction_cum': 'avg_transaction_before'\n",
    "}, inplace=True)\n",
    "\n",
    "# --- Passo 8: preencher valores NaN se necessÃ¡rio (exemplo: zeros)\n",
    "merged['transaction_count_before'].fillna(0, inplace=True)\n",
    "merged['total_transacted_before'].fillna(0, inplace=True)\n",
    "merged['avg_transaction_before'].fillna(0, inplace=True)\n",
    "\n",
    "# --- Resultado final\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6c187-eb6a-4976-bcb7-46c04ba9f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino.groupby(['event']).count()[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3dc32d-40b2-4be3-bfc7-77cb41e0616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino[['age', 'gender', 'credit_card_limit',\n",
    "      'tempo_de_casa', 'event', 'time_since_test_start', 'offer_id']]\n",
    "\n",
    "Normalmente, uma oferta sÃ³ Ã© considerada â€œcompletadaâ€ se a transaÃ§Ã£o relacionada ocorreu â€” ou seja, o evento offer completed costuma vir associado a uma transaÃ§Ã£o que validou a conclusÃ£o daquela oferta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1169f-39d3-4034-8616-8277072f3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que base_treino Ã© o DataFrame original\n",
    "\n",
    "# 1. Ofertas recebidas (com dados necessÃ¡rios)\n",
    "offers = base_treino[base_treino['event'] == 'offer received'][['id', 'offer_id', 'time_since_test_start', 'offer_duration']]\n",
    "offers = offers.rename(columns={'time_since_test_start': 'offer_start'})\n",
    "\n",
    "# 2. Calcular o fim da validade da \n",
    "\n",
    "\n",
    "offers['offer_end'] = offers['offer_start'] + offers['offer_duration']\n",
    "\n",
    "# 3. TransaÃ§Ãµes\n",
    "transacoes = base_treino[base_treino['event'] == 'transaction'][['id', 'time_since_test_start', 'amount']].copy()\n",
    "transacoes['amount'] = pd.to_numeric(transacoes['amount'], errors='coerce')\n",
    "transacoes = transacoes.dropna(subset=['amount'])\n",
    "\n",
    "# 4. Merge para relacionar transaÃ§Ãµes Ã s ofertas do mesmo cliente\n",
    "merged = pd.merge(transacoes, offers, on='id', how='inner')\n",
    "\n",
    "# 5. Filtrar sÃ³ transaÃ§Ãµes que ocorreram entre o inÃ­cio e fim da oferta\n",
    "merged = merged[(merged['time_since_test_start'] >= merged['offer_start']) & \n",
    "                (merged['time_since_test_start'] <= merged['offer_end'])]\n",
    "\n",
    "# 6. Agora, para cada oferta recebida, verificar se teve transaÃ§Ã£o (ou seja, foi completada)\n",
    "# Criar flag de oferta completada\n",
    "merged['offer_completed'] = 1\n",
    "\n",
    "# 7. Agregar para ter oferta e se foi completada (1 ou 0)\n",
    "completed_flag = merged.groupby(['id', 'offer_id', 'offer_start']).agg({'offer_completed': 'max'}).reset_index()\n",
    "\n",
    "# 8. Juntar com a tabela original de ofertas para garantir todas, preencher 0 para nÃ£o completadas\n",
    "final_offers = pd.merge(offers, completed_flag, on=['id', 'offer_id', 'offer_start'], how='left')\n",
    "final_offers['offer_completed'] = final_offers['offer_completed'].fillna(0).astype(int)\n",
    "\n",
    "# final_offers tem a variÃ¡vel target pronta para modelagem\n",
    "print(final_offers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b855ae3-7277-4771-b3a0-61e86e11452b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bd4f8-add0-4b46-9288-80760319cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc310aca-632e-496d-9443-8e8db8ebc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
